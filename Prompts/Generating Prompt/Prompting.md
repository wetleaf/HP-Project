# Prompting

## Introduction

Prompting depends on the task. 

For **generation tasks**, the prompt should be a description of the task.

By [$reference^1$](#best-practices), the prompt should contain the following:

- A description of the task
- Instructions for the model
- Formatting instructions
- Examples

## Methodology

Since the project's requirement is to generate a worksheet, we have divided the prompt into two parts:-

- Question type: Using the topic title, LLMs can generate multiple question types.
- Question content: Once the type of question is decided, LLMs can now generate multiple questions of that type with different modes, such as fill-in-the-blank, multiple-choice, etc.

For an example, if the topic is "Clocks", then the LLMs can generate a question type such as "Tell Time" and then generate questions of that type.

## Prompt Format

### Part 1: Question Type

The prompt format for generating question type is as follows:

```markdown
```

Example prompt for the topic "Clocks" would be:
```markdown
```

This prompt would generate question types such as:
```markdown
```

### Part 2: Question Content

The prompt format for generating question content is as follows:

```markdown
```

Example prompt for the topic "Clocks" and question type "Tell Time" would be:
```markdown
```

This prompt would generate questions such as:
```markdown
```

## Pipeline

The pipeline for generating questions is as follows:

1. Get the topic title, number of questions, difficulty level, age group, and country of the students from the user.
2. Generate the question type response using the topic title.
3. Parse the question type response to get the question type.
4. Generate the question content prompt using the topic title and question type.
5. Parse the question content response to get the questions.

## Limitations

We right now have no access to the OpenAI API(Paid) nor to a workstation GPU, All we can do is to test the prompt and see if it generates the desired output through the web interface.

Due to this, we cannot:
- Test the answers generated by the GPT LLMs. All we can do is to test the prompt and see if it generates the desired output through the web interface. 
- Test open source LLMs such as LLama on our own.
- Fine-tune on a dataset of questions and answers

## References

Note that below are clickable links to the OpenAI documentation.

#### [Best Practices](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)

#### [Fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)

#### [Preparing Datasets](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)